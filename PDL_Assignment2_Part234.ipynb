{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load Processed Review File"
      ],
      "metadata": {
        "id": "vywKunR937x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Electronics.json.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPLaexayTvNA",
        "outputId": "539041c0-1618-48cd-c2e5-4758b7ffef0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-18 21:59:53--  https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Electronics.json.gz\n",
            "Resolving datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)... 132.239.8.30\n",
            "Connecting to datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)|132.239.8.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3322874357 (3.1G) [application/x-gzip]\n",
            "Saving to: ‘Electronics.json.gz’\n",
            "\n",
            "Electronics.json.gz 100%[===================>]   3.09G  47.5MB/s    in 92s     \n",
            "\n",
            "2024-12-18 22:01:25 (34.4 MB/s) - ‘Electronics.json.gz’ saved [3322874357/3322874357]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(review):\n",
        "    text = review.lower()  # Lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Step 1: Load reviews and ratings\n",
        "file_path = \"Electronics.json.gz\"\n",
        "reviews = []\n",
        "ratings = []\n",
        "\n",
        "with gzip.open(file_path, 'rt') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        review = json.loads(line)\n",
        "        if 'reviewText' in review and 'overall' in review:\n",
        "            reviews.append(review['reviewText'])\n",
        "            ratings.append(review['overall'])\n",
        "        if i >= 100000:  # Limit to first 100,000 reviews\n",
        "            break\n",
        "\n",
        "print(f\"Loaded {len(reviews)} reviews and {len(ratings)} ratings.\")\n",
        "\n",
        "# Step 2: Randomly select 10% of the data\n",
        "subset_size = int(0.1 * len(reviews))  # Calculate 10% of the total data\n",
        "subset_indices = random.sample(range(len(reviews)), subset_size)\n",
        "\n",
        "# Create the 10% subset\n",
        "reviews_subset = [reviews[i] for i in subset_indices]\n",
        "ratings_subset = [ratings[i] for i in subset_indices]\n",
        "\n",
        "print(f\"Subset size: {len(reviews_subset)} reviews and {len(ratings_subset)} ratings.\")\n",
        "\n",
        "# Step 3: Preprocess the reviews\n",
        "print(\"Starting parallel preprocessing...\")\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    processed_reviews_subset = list(tqdm(executor.map(preprocess_text, reviews_subset), total=len(reviews_subset)))\n",
        "\n",
        "print(\"Preprocessing complete.\")\n",
        "\n",
        "# Step 4: Combine into a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'original_review': reviews_subset,\n",
        "    'processed_review': processed_reviews_subset,\n",
        "    'overall': ratings_subset\n",
        "})\n",
        "\n",
        "# Step 5: Save the final DataFrame\n",
        "data.to_csv('processed_reviews_10percent.csv', index=False)\n",
        "print(\"File saved: processed_reviews_10percent.csv\")\n",
        "\n",
        "# Verify\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX9nVP0YcdAe",
        "outputId": "048bb334-59d1-4128-c984-21f7eba129ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 99987 reviews and 99987 ratings.\n",
            "Subset size: 9998 reviews and 9998 ratings.\n",
            "Starting parallel preprocessing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9998/9998 [00:04<00:00, 2141.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete.\n",
            "File saved: processed_reviews_10percent.csv\n",
            "                                     original_review  \\\n",
            "0  Yes, you get what you pay for, but I've had ch...   \n",
            "1  Best $5 I have ever spent.  I should have boug...   \n",
            "2                                         not usable   \n",
            "3            Always useful, this one for camera use.   \n",
            "4  Great product, able to hide it when i don't ne...   \n",
            "\n",
            "                                    processed_review  overall  \n",
            "0  yes get pay ive cheap headphones last many yea...      2.0  \n",
            "1  best 5 ever spent bought 2 great listening lat...      5.0  \n",
            "2                                             usable      1.0  \n",
            "3                       always useful one camera use      5.0  \n",
            "4  great product able hide dont need break im goi...      5.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Load the Data\n",
        "data = pd.read_csv('processed_reviews_10percent.csv')\n",
        "\n",
        "# Step 2: Create Sentiment Labels\n",
        "def assign_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 0  # Positive\n",
        "    elif rating <= 2:\n",
        "        return 1  # Negative\n",
        "    else:\n",
        "        return 2  # Neutral\n",
        "\n",
        "data['sentiment'] = data['overall'].apply(assign_sentiment)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data[['processed_review', 'sentiment']]"
      ],
      "metadata": {
        "id": "QWe0Bj0phqP7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split into Train, Validation, and Test\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Y-eD6zh2j-",
        "outputId": "275cf10f-3b64-433b-b320-353c4a5aca90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 7198, Validation size: 800, Test size: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Vocabulary (from Part 1)\n",
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "\n",
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    tokens = text.split()  # Preprocessed reviews are space-separated\n",
        "    indices = [vocab.get(token, UNK_IDX) for token in tokens]\n",
        "    return indices[:max_len] + [PAD_IDX] * (max_len - len(indices))"
      ],
      "metadata": {
        "id": "joiUbcd8h4rG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Step 1: Clean the 'processed_review' column\n",
        "train_data = train_data.dropna(subset=['processed_review']).reset_index(drop=True)\n",
        "train_data['processed_review'] = train_data['processed_review'].astype(str)\n",
        "\n",
        "# Step 2: Build the vocabulary\n",
        "min_word_count = 5  # Words must appear at least 5 times to be included\n",
        "counter = Counter()\n",
        "\n",
        "# Count word frequencies in the 'processed_review' column\n",
        "for review in train_data['processed_review']:\n",
        "    counter.update(review.split())\n",
        "\n",
        "# Create the vocabulary\n",
        "vocab = {word: idx + 2 for idx, (word, count) in enumerate(counter.items()) if count >= min_word_count}\n",
        "vocab['<PAD>'] = 0  # Padding index\n",
        "vocab['<UNK>'] = 1  # Unknown words index\n",
        "\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUSwUgZP4FMp",
        "outputId": "07988467-8806-4e7a-8670-c5457d2dbd1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 4758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample vocabulary:\", list(vocab.items())[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deeocti24KnZ",
        "outputId": "bf5e837e-5c59-4876-b4f4-a35b6ebf66ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vocabulary: [('bought', 2), ('noise', 3), ('cancelling', 4), ('headset', 5), ('use', 6), ('airplane', 7), ('make', 8), ('easier', 9), ('control', 10), ('volume', 11)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the 'processed_review' column in all datasets\n",
        "for df in [train_data, val_data, test_data]:\n",
        "    df.dropna(subset=['processed_review'], inplace=True)  # Drop rows with NaN\n",
        "    df['processed_review'] = df['processed_review'].astype(str).str.strip()  # Convert to strings and strip whitespaces\n",
        "    df = df.reset_index(drop=True)  # Reset index after dropping rows"
      ],
      "metadata": {
        "id": "BjHbHdBN4pnI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train data sample:\")\n",
        "print(train_data['processed_review'].head())\n",
        "\n",
        "print(\"Validation data sample:\")\n",
        "print(val_data['processed_review'].head())\n",
        "\n",
        "print(\"Test data sample:\")\n",
        "print(test_data['processed_review'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLQWr_ZP4tF9",
        "outputId": "e4eec1ea-6cbe-4288-a539-50f6251c93dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data sample:\n",
            "0    bought noise cancelling headset use airplane m...\n",
            "1           needed review cassette tapes product ideal\n",
            "2    mighty 1200 live forever yes sound better cd p...\n",
            "3       unable justify rating used need longer concern\n",
            "4    typical romance novel nothing fantastic easy r...\n",
            "Name: processed_review, dtype: object\n",
            "Validation data sample:\n",
            "7614    good job holding 55 lcd like way swivels folds...\n",
            "9314    nice set wires love purchase better ones buy 9...\n",
            "8785    set easy works well one got spectrum problem f...\n",
            "8746                                                sweet\n",
            "9207    beauty card rj45 receptacle attached permanent...\n",
            "Name: processed_review, dtype: object\n",
            "Test data sample:\n",
            "4122    theyre perfect far appear sturdy sound great t...\n",
            "4065    perfect set tools need flashlight overall sati...\n",
            "1731    bought child tested first normally earphones h...\n",
            "4740                                    satisfied product\n",
            "6391    purchased tv mount tv motorhome several friend...\n",
            "Name: processed_review, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    if not isinstance(text, str):  # Ensure text is a string\n",
        "        text = \"\"\n",
        "    tokens = text.split()\n",
        "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    return indices[:max_len] + [vocab['<PAD>']] * (max_len - len(indices))\n"
      ],
      "metadata": {
        "id": "cQiqG-hF4wPU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "# Step 5: Dataset Class\n",
        "# Ensure SentimentDataset class is defined correctly\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, data, vocab, max_len=50):\n",
        "        self.vocab = vocab\n",
        "        self.reviews = [text_to_indices(text, vocab, max_len) for text in data['processed_review']]\n",
        "        self.labels = data['sentiment'].values.astype(int)  # Ensure labels are integers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        processed_review = [\n",
        "            self.vocab.get(token, 1)  # Use 1 as UNK_IDX if token is not in vocab\n",
        "            for token in self.reviews[idx]\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Convert to tensors\n",
        "        return torch.tensor(processed_review, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Recreate datasets and DataLoaders\n",
        "train_dataset = SentimentDataset(train_data, vocab, max_len=50)\n",
        "val_dataset = SentimentDataset(val_data, vocab, max_len=50)\n",
        "test_dataset = SentimentDataset(test_data, vocab, max_len=50)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=2)"
      ],
      "metadata": {
        "id": "GDkgxxdM3VAq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output logits for 3 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "        embedded = embedded.mean(dim=1)  # Average over sequence length\n",
        "        hidden = self.relu(self.fc1(embedded))\n",
        "        output = self.fc2(hidden)  # Shape: [batch_size, num_classes]\n",
        "        return output"
      ],
      "metadata": {
        "id": "O9QVtQNr5xpT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for reviews, labels in train_loader:\n",
        "    print(\"Reviews shape:\", reviews.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "    print(\"Min index:\", reviews.min(), \"Max index:\", reviews.max())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG-EkF1PENnf",
        "outputId": "101cd8f8-ba0c-4589-e201-c422943e9c39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews shape: torch.Size([32, 50])\n",
            "Labels shape: torch.Size([32])\n",
            "Min index: tensor(1) Max index: tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample vocab entries:\", list(vocab.items())[:10])  # Print the first 10 vocab entries\n",
        "print(\"Vocabulary size:\", len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxIxRWPpGNnO",
        "outputId": "cedb5d91-dc48-449d-9435-cf369bf6bafc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vocab entries: [('bought', 2), ('noise', 3), ('cancelling', 4), ('headset', 5), ('use', 6), ('airplane', 7), ('make', 8), ('easier', 9), ('control', 10), ('volume', 11)]\n",
            "Vocabulary size: 4758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample processed review:\", train_data['processed_review'].iloc[0])\n",
        "tokens = train_data['processed_review'].iloc[0].split()\n",
        "print(\"Tokens in sample review:\", tokens)\n",
        "\n",
        "# Check if tokens are in vocab\n",
        "for token in tokens:\n",
        "    if token in vocab:\n",
        "        print(f\"Token '{token}' is in vocab with index {vocab[token]}\")\n",
        "    else:\n",
        "        print(f\"Token '{token}' is NOT in vocab\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUEygz4qGP-f",
        "outputId": "f84cbe6b-82eb-43a6-a12a-9dbd3bc2d246"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample processed review: bought noise cancelling headset use airplane make easier control volume without dig ipod switches titlestracks works fine downside cordcable wish cable could retracted desired length\n",
            "Tokens in sample review: ['bought', 'noise', 'cancelling', 'headset', 'use', 'airplane', 'make', 'easier', 'control', 'volume', 'without', 'dig', 'ipod', 'switches', 'titlestracks', 'works', 'fine', 'downside', 'cordcable', 'wish', 'cable', 'could', 'retracted', 'desired', 'length']\n",
            "Token 'bought' is in vocab with index 2\n",
            "Token 'noise' is in vocab with index 3\n",
            "Token 'cancelling' is in vocab with index 4\n",
            "Token 'headset' is in vocab with index 5\n",
            "Token 'use' is in vocab with index 6\n",
            "Token 'airplane' is in vocab with index 7\n",
            "Token 'make' is in vocab with index 8\n",
            "Token 'easier' is in vocab with index 9\n",
            "Token 'control' is in vocab with index 10\n",
            "Token 'volume' is in vocab with index 11\n",
            "Token 'without' is in vocab with index 12\n",
            "Token 'dig' is NOT in vocab\n",
            "Token 'ipod' is in vocab with index 14\n",
            "Token 'switches' is in vocab with index 15\n",
            "Token 'titlestracks' is NOT in vocab\n",
            "Token 'works' is in vocab with index 17\n",
            "Token 'fine' is in vocab with index 18\n",
            "Token 'downside' is in vocab with index 19\n",
            "Token 'cordcable' is NOT in vocab\n",
            "Token 'wish' is in vocab with index 21\n",
            "Token 'cable' is in vocab with index 22\n",
            "Token 'could' is in vocab with index 23\n",
            "Token 'retracted' is NOT in vocab\n",
            "Token 'desired' is in vocab with index 25\n",
            "Token 'length' is in vocab with index 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SentimentClassifier(vocab_size=len(vocab), embedding_dim=50, hidden_dim=100, output_dim=3, pad_idx=0)\n",
        "model = model.to(device)  # Move the model to the selected device\n",
        "\n",
        "for reviews, labels in train_loader:\n",
        "    # Move input tensors to the model's device\n",
        "    reviews, labels = reviews.to(device), labels.to(device)\n",
        "    print(\"Input reviews device:\", reviews.device)  # Debug check\n",
        "    print(\"Labels device:\", labels.device)\n",
        "    print(\"Model device:\", next(model.parameters()).device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(reviews)\n",
        "    print(\"Outputs device:\", outputs.device)  # Confirm outputs are on the same device\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEZZz4u4Emao",
        "outputId": "9b9a488d-eb5b-454c-d88b-42b46614a03c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input reviews device: cuda:0\n",
            "Labels device: cuda:0\n",
            "Model device: cuda:0\n",
            "Outputs device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(vocab)  # Size of vocabulary\n",
        "embedding_dim = 128      # Dimension of word embeddings\n",
        "hidden_dim = 128         # Hidden dimension size\n",
        "output_dim = 3           # Number of classes: Positive, Neutral, Negative\n",
        "PAD_IDX = vocab['<PAD>'] # Padding index\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SentimentClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, PAD_IDX).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "GhjwyjnK5zna"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0\n",
        "\n",
        "        for reviews, labels in train_loader:\n",
        "            reviews, labels = reviews.to(device), labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(reviews)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predictions, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for reviews, labels in val_loader:\n",
        "                reviews, labels = reviews.to(device), labels.to(device)\n",
        "                predictions = model(reviews)\n",
        "                loss = criterion(predictions, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Compute accuracy\n",
        "                preds = torch.argmax(predictions, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "        print(f\"Validation Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "id": "tspcA92x57m_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGyd9kHI-jcl",
        "outputId": "6e8823b8-6ed0-4516-d446-9a20d03752aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 2  # Accumulate gradients for 2 batches\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for i, (reviews, labels) in enumerate(train_loader):\n",
        "    reviews, labels = reviews.to(device), labels.to(device)\n",
        "    outputs = model(reviews)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    if (i + 1) % gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "KBGNTtf9_IIF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in train data:\", train_data['sentiment'].unique())\n",
        "print(\"Unique labels in validation data:\", val_data['sentiment'].unique())\n",
        "print(\"Unique labels in test data:\", test_data['sentiment'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5rI0zGmAM-q",
        "outputId": "61ee975b-ecb6-4d97-a402-f2eb237e765a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in train data: [0 2 1]\n",
            "Unique labels in validation data: [0 1 2]\n",
            "Unique labels in test data: [0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5)"
      ],
      "metadata": {
        "id": "McsgovAH5_Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b7dbc2-ff85-47e7-d58a-cba96cf9db7b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 0.6622\n",
            "Validation Loss: 0.6762\n",
            "Validation Accuracy: 0.7779\n",
            "Epoch 2/5\n",
            "Train Loss: 0.6638\n",
            "Validation Loss: 0.6746\n",
            "Validation Accuracy: 0.7779\n",
            "Epoch 3/5\n",
            "Train Loss: 0.6630\n",
            "Validation Loss: 0.6761\n",
            "Validation Accuracy: 0.7779\n",
            "Epoch 4/5\n",
            "Train Loss: 0.6610\n",
            "Validation Loss: 0.6764\n",
            "Validation Accuracy: 0.7779\n",
            "Epoch 5/5\n",
            "Train Loss: 0.6619\n",
            "Validation Loss: 0.6750\n",
            "Validation Accuracy: 0.7779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for reviews, labels in test_loader:\n",
        "            reviews, labels = reviews.to(device), labels.to(device)\n",
        "            predictions = model(reviews)\n",
        "            preds = torch.argmax(predictions, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "W_KNcEYm6KUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ea5013-9652-47f4-97dc-1a4515e8c88e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Extensions to the Model (6 Points)"
      ],
      "metadata": {
        "id": "9ihodvG0-qhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different embedding dimensions\n",
        "for embedding_dim in [50, 100, 200]:\n",
        "    print(f\"Training with embedding dimension: {embedding_dim}\")\n",
        "\n",
        "    model = SentimentClassifier(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=embedding_dim,  # Variable embedding dimension\n",
        "        hidden_dim=100,              # First hidden layer dimension\n",
        "        output_dim=3,                 # Number of classes\n",
        "        pad_idx=0                     # Padding index\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for reviews, labels in train_loader:\n",
        "            reviews, labels = reviews.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(reviews)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for reviews, labels in val_loader:\n",
        "            reviews, labels = reviews.to(device), labels.to(device)\n",
        "            outputs = model(reviews)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Validation Accuracy with embedding dim {embedding_dim}: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XEMLXO1-n8M",
        "outputId": "f0e6660c-597f-434c-bd46-f86e813812a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with embedding dimension: 50\n",
            "Epoch 1, Loss: 0.6727568133672078\n",
            "Epoch 2, Loss: 0.6617682127157847\n",
            "Epoch 3, Loss: 0.6619373570548164\n",
            "Epoch 4, Loss: 0.6606744694709777\n",
            "Epoch 5, Loss: 0.6600558445188734\n",
            "Validation Accuracy with embedding dim 50: 0.7779\n",
            "Training with embedding dimension: 100\n",
            "Epoch 1, Loss: 0.6715475738048553\n",
            "Epoch 2, Loss: 0.6634536892175674\n",
            "Epoch 3, Loss: 0.6610300470723046\n",
            "Epoch 4, Loss: 0.6612279424402449\n",
            "Epoch 5, Loss: 0.6602384403016832\n",
            "Validation Accuracy with embedding dim 100: 0.7779\n",
            "Training with embedding dimension: 200\n",
            "Epoch 1, Loss: 0.6705974475542704\n",
            "Epoch 2, Loss: 0.664615275727378\n",
            "Epoch 3, Loss: 0.6634033865398831\n",
            "Epoch 4, Loss: 0.6627227024237314\n",
            "Epoch 5, Loss: 0.6626748592323727\n",
            "Validation Accuracy with embedding dim 200: 0.7779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding a Hidden Layer\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, pad_idx):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)  # New hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x).mean(dim=1)  # Average embeddings\n",
        "        hidden1 = self.relu(self.fc1(embedded))\n",
        "        hidden2 = self.relu(self.fc2(hidden1))  # Pass through second hidden layer\n",
        "        output = self.fc3(hidden2)\n",
        "        return output\n",
        "\n",
        "embedding_dim=100\n",
        "model = SentimentClassifier(\n",
        "vocab_size=len(vocab),\n",
        "embedding_dim=embedding_dim,\n",
        "hidden_dim1=128,  # First hidden layer\n",
        "hidden_dim2=64,   # Second hidden layer\n",
        "output_dim=3,\n",
        "pad_idx=0\n",
        ").to(device)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):  # Train for 5 epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for reviews, labels in train_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(reviews)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluate on validation set\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for reviews, labels in val_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        outputs = model(reviews)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy with embedding dim {embedding_dim}: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z3GQqGA-zyR",
        "outputId": "ab2917a5-65d9-4241-a5b2-01c90227d8a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9999055385589599\n",
            "Epoch 2, Loss: 0.9999153282907274\n",
            "Epoch 3, Loss: 0.9998822771178352\n",
            "Epoch 4, Loss: 0.9998394544919332\n",
            "Epoch 5, Loss: 0.9999085876676771\n",
            "Validation Accuracy with embedding dim 100: 0.7779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze Embeddings\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, pad_idx):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x).mean(dim=1)  # Average embeddings\n",
        "        hidden1 = self.relu(self.fc1(embedded))\n",
        "        hidden2 = self.relu(self.fc2(hidden1))\n",
        "        output = self.fc3(hidden2)\n",
        "        return output\n",
        "\n",
        "#Train the model\n",
        "model = SentimentClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=100,\n",
        "    hidden_dim1=256,\n",
        "    hidden_dim2=128,\n",
        "    output_dim=3,\n",
        "    pad_idx=0\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):  # Train for 5 epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for reviews, labels in train_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(reviews)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "#Evaluate the model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for reviews, labels in val_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        outputs = model(reviews)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy with frozen embeddings: {correct / total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDiU7B7EE5kC",
        "outputId": "c3807461-df94-4373-dee2-21b6e903b484"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6695410244994693\n",
            "Epoch 2, Loss: 0.6657237780094146\n",
            "Epoch 3, Loss: 0.663375201092826\n",
            "Epoch 4, Loss: 0.6616350891855028\n",
            "Epoch 5, Loss: 0.6614855986171299\n",
            "Validation Accuracy with frozen embeddings: 0.7779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation accuracy remains the same (07779) for both frozen and fine-tuned embeddings.\n",
        "\n",
        "The pre-trained embeddings may already represent the input text well enough for the task. Further tuning doesn't add significant value.\n",
        "\n",
        "Freezing the pre-trained embeddings is a more computationally efficient choice since it yields the same performance as fine-tuning (0.7779 validation accuracy) while requiring fewer parameters to update."
      ],
      "metadata": {
        "id": "lP1Ssmqd3CN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Comparison with Random Embeddings"
      ],
      "metadata": {
        "id": "8-5g2kfeIJj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a new model with random embeddings\n",
        "class SentimentClassifierRandom(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super(SentimentClassifierRandom, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # Initialize with random weights\n",
        "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x).mean(dim=1)  # Average embeddings\n",
        "        hidden = self.relu(self.fc1(embedded))\n",
        "        output = self.fc2(hidden)\n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "model_random = SentimentClassifierRandom(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=100,\n",
        "    hidden_dim=128,\n",
        "    output_dim=3,\n",
        "    pad_idx=0\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_random.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Train for 5 epochs\n",
        "    model_random.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for reviews, labels in train_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_random(reviews)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "model_random.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for reviews, labels in test_loader:\n",
        "        reviews, labels = reviews.to(device), labels.to(device)\n",
        "        outputs = model_random(reviews)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy with Random Embeddings: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vKy3MUQIOsg",
        "outputId": "44330e65-aceb-4da9-eb07-daccae38584f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7078133289019267\n",
            "Epoch 2, Loss: 0.6603058771292368\n",
            "Epoch 3, Loss: 0.6597366638978323\n",
            "Epoch 4, Loss: 0.6598447374502818\n",
            "Epoch 5, Loss: 0.6597351370917426\n",
            "Test Accuracy with Random Embeddings: 0.7944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output, the test accuracy for both models—one using pre-trained embeddings and the other using randomly initialized embeddings—was 0.7944. The sentiment classification task might not require complex word representations, so the model can achieve good accuracy with random embeddings.\n",
        "\n",
        "Either that or, the training was unsuccessful."
      ],
      "metadata": {
        "id": "l6I8knxs1jiR"
      }
    }
  ]
}