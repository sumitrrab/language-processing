{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXf1Rn6c0YpR"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Electronics.json.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m985gsxF0ueR",
        "outputId": "890b1bd1-263c-431b-ba3f-516cd71f2e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-16 21:15:18--  https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Electronics.json.gz\n",
            "Resolving datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)... 132.239.8.30\n",
            "Connecting to datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)|132.239.8.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3322874357 (3.1G) [application/x-gzip]\n",
            "Saving to: ‘Electronics.json.gz’\n",
            "\n",
            "Electronics.json.gz 100%[===================>]   3.09G  36.9MB/s    in 81s     \n",
            "\n",
            "2024-12-16 21:16:40 (39.1 MB/s) - ‘Electronics.json.gz’ saved [3322874357/3322874357]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Step 1: Download and preprocess the dataset\n",
        "file_path = \"Electronics.json.gz\"\n",
        "reviews = []\n",
        "\n",
        "with gzip.open(file_path, 'rt') as f:\n",
        "    for line in f:\n",
        "        review = json.loads(line)\n",
        "        if 'reviewText' in review:\n",
        "            reviews.append(review['reviewText'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkA1aftg0ahs",
        "outputId": "83a25743-3347-4ea1-a83d-0e5c096d3612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt_tab\")\n",
        "# Downsample dataset\n",
        "N = 100000\n",
        "selected_reviews = reviews[:N]\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "# Preprocess all reviews\n",
        "processed_reviews = [preprocess_text(review) for review in selected_reviews]\n",
        "\n",
        "# Step 2: Vocabulary creation\n",
        "min_word_count = 5\n",
        "word_counts = Counter(word for review in processed_reviews for word in review)\n",
        "vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items()) if count >= min_word_count}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GoliHj_0yUy",
        "outputId": "f70129b5-fa22-47f6-de46-8833305db93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create word_to_index and index_to_word mappings\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Step 3: Generate training data (center-context pairs)\n",
        "window_size = 2\n",
        "def generate_training_data(processed_reviews, word_to_index, window_size):\n",
        "    pairs = []\n",
        "    for review in processed_reviews:\n",
        "        indices = [word_to_index[word] for word in review if word in word_to_index]\n",
        "        for center_idx in range(len(indices)):\n",
        "            for offset in range(-window_size, window_size + 1):\n",
        "                context_idx = center_idx + offset\n",
        "                if context_idx < 0 or context_idx >= len(indices) or center_idx == context_idx:\n",
        "                    continue\n",
        "                pairs.append((indices[center_idx], indices[context_idx]))\n",
        "    return pairs\n"
      ],
      "metadata": {
        "id": "BR2yxifU04GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_pairs = generate_training_data(processed_reviews, word_to_index, window_size)\n",
        "\n",
        "# Step 4: Simplified Word2Vec model\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, center_words):\n",
        "        embeddings = self.embedding(center_words)\n",
        "        output = self.output_layer(embeddings)\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 50\n",
        "batch_size = 1024\n",
        "epochs = 5\n",
        "\n",
        "# DataLoader for batching\n",
        "def create_dataloader(training_pairs, batch_size):\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.tensor([pair[0] for pair in training_pairs], dtype=torch.long),\n",
        "        torch.tensor([pair[1] for pair in training_pairs], dtype=torch.long)\n",
        "    )\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloader = create_dataloader(training_pairs, batch_size)\n"
      ],
      "metadata": {
        "id": "-gvHU86P07i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, optimizer, and loss function\n",
        "model = Word2Vec(vocab_size, embed_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for center, context in dataloader:\n",
        "        center, context = center.to(device), context.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(center)\n",
        "        loss = criterion(output, context)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK9R3aX11BNL",
        "outputId": "a7630f48-9e91-49be-f2a0-052296642ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 87897.6557\n",
            "Epoch 2/5, Loss: 84426.3095\n",
            "Epoch 3/5, Loss: 83602.1010\n",
            "Epoch 4/5, Loss: 83138.6846\n",
            "Epoch 5/5, Loss: 82829.3236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDZJj4ja3fXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
